# Intent_Recognition_with_BERT-TPU
Intent Recognition with Bert Model in Google Colab TPU using Tensorflow

BERT(Bidirectional Encoder Representations from Transformers) is a pretrained model for Natural Language Processing(NLP) tasks. It was introduced by the Google AI team in October 2018. It is trained on Wikipedia and a Corpus dataset; it knows the language and context, which is quite decent. It has two versions: Base(12 encoders) and Large (24 encoders). BERT is built on top of multiple clever ideas by the NLP community. Some examples are ELMo, the OpenAI Transformer, and the Transformer.

If you want to get more information about BERT, follow the following links.
- [See BERT on Paper](https://arxiv.org/pdf/1810.04805.pdf)
- [See BERT on GitHub](https://github.com/google-research/bert)
- [See BERT on TensorHub](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1)
